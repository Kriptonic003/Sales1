# DistilBERT Implementation Setup Guide

This guide walks through implementing DistilBERT for improved sentiment classification in your AI Sales Loss Prediction System.

## What Changed

- **Old approach**: Heuristic keyword matching (limited accuracy)
- **New approach**: DistilBERT pre-trained transformer (90%+ accuracy)

### Files Modified/Created

1. âœ… `backend/ml/sentiment_classifier.py` (NEW) - DistilBERT wrapper class
2. âœ… `backend/ml/pipeline.py` (UPDATED) - Uses DistilBERT instead of heuristics
3. âœ… `backend/requirements.txt` (UPDATED) - Added transformers, torch, safetensors

## Step-by-Step Setup

### Step 1: Update Python Dependencies

```bash
cd backend

# Upgrade pip (recommended)
pip install --upgrade pip

# Install new dependencies
pip install -r requirements.txt
```

**What's being installed:**

- `transformers>=4.30.0` - Hugging Face transformer models library
- `torch>=2.0.0` - PyTorch deep learning framework (core engine)
- `safetensors` - Safe model serialization format

**â±ï¸ Installation Time:** 5-10 minutes (PyTorch is large ~800MB)

**ðŸ’¡ Tip:** First run downloads the DistilBERT model (~270MB) automatically. Keep internet connection active.

### Step 2: Set Environment Variables

Create/update your `.env` file in the `backend/` directory:

```bash
# YouTube API (required)
YOUTUBE_API_KEY=your_youtube_api_key_here

# Gemini API (optional, for chatbot)
GEMINI_API_KEY=your_gemini_api_key_here

# (Optional) Force CPU mode if GPU is unavailable
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb=512
```

### Step 3: Verify Installation

Test that everything works:

```bash
# In backend directory
python -c "from ml.sentiment_classifier import DistilBERTSentimentClassifier;
clf = DistilBERTSentimentClassifier();
print(clf.classify('This product is amazing!'))"
```

**Expected output:**

```
âœ“ DistilBERT Sentiment Classifier loaded on CPU
('positive', 0.9987...)
```

### Step 4: Start the Backend Server

```bash
# From backend directory
uvicorn main:app --reload --port 8000
```

**First startup takes longer** because the model is being initialized. Subsequent restarts are faster.

### Step 5: Test Sentiment Analysis Endpoint

Use curl or API client to test:

```bash
curl -X POST "http://localhost:8000/analyze-sentiment" \
  -H "Content-Type: application/json" \
  -d '{
    "product_name": "iPhone",
    "brand_name": "Apple",
    "platform": "youtube",
    "start_date": "2024-02-01",
    "end_date": "2024-02-18"
  }'
```

You should see much better sentiment scores now!

### Step 6: Start Frontend

```bash
cd frontend
npm run dev -- --host --port 5173
```

Open `http://localhost:5173` in your browser.

## Performance & Hardware

### Minimum Requirements

- **RAM:** 4GB (CPU mode)
- **Storage:** 1GB (for model + packages)
- **CPU:** Modern processor (any within last 5 years)

### GPU Acceleration (Optional but Recommended)

If you have an NVIDIA GPU:

```bash
# Install CUDA-enabled PyTorch instead
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# System will auto-detect CUDA and use GPU
# Output will show: "âœ“ DistilBERT Sentiment Classifier loaded on CUDA"
```

**Speed comparison:**

- CPU: ~1 comment per second
- GPU: ~10-50 comments per second (depending on GPU)

### No GPU? Use CPU Mode

It works fine! Just slower. The default installation uses CPU automatically.

## How DistilBERT Works

1. **Text Input** â†’ "This product is terrible, waste of money"
2. **Tokenization** â†’ Break into tokens the model understands
3. **Transformer Layers** â†’ Process through 6 neural network layers
4. **Classification Head** â†’ Output sentiment probability
5. **Result** â†’ `"negative", confidence: 0.995`

## API Changes

### Before (Heuristic)

```
Comment: "i love this"  â†’ sentiment: 0.6 (guessed)
Comment: "this is bad"  â†’ sentiment: -0.7 (guessed)
```

### After (DistilBERT)

```
Comment: "i love this"  â†’ sentiment: 0.998 (actual language understanding)
Comment: "i hate this awesome phone" â†’ sentiment: -0.8 (understands sarcasm better)
```

## Troubleshooting

### âŒ Error: "Model not found" or "Connection timeout"

**Cause:** The model couldn't download from Hugging Face
**Solution:**

```bash
# Download offline (once you have internet)
python -c "from transformers import AutoModelForSequenceClassification;
AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"

# Then it's cached locally
```

### âŒ Error: "CUDA out of memory"

**Solution:** Reduce batch size or use CPU:

```python
# In main.py, change:
classifier = DistilBERTSentimentClassifier(device='cpu')
```

### âŒ Server slow on first run

**Normal!** First startup loads the 270MB model. Subsequent restarts use cache.

### âŒ Different sentiment predictions than before

**Expected!** DistilBERT is smarter. It reads actual text meaning, not just keywords. Old heuristics might break.

- Review predictions on key comments
- Adjust thresholds if needed (see `backend/ml/pipeline.py`)

## Customization

### Change Confidence Threshold

In `backend/ml/pipeline.py`:

```python
if score > 0.3:  # 30% confidence threshold (was 20%)
    label = "positive"
elif score < -0.3:
    label = "negative"
```

### Use a Different Model

Replace model name in `backend/ml/sentiment_classifier.py`:

```python
self.model_name = "distilbert-base-uncased-finetuned-sst-2-english"  # Change this

# Options:
# "cardiffnlp/twitter-roberta-base-sentiment" (Twitter-tuned)
# "facebook/bart-large-mnli" (Multi-label)
# "distilbert-base-multilingual-uncased-finetuned-sst-2-en" (Multilingual)
```

### Improve Accuracy Further

For production, consider fine-tuning on your specific data:

- Collect labeled examples from your domain
- Fine-tune the model using Hugging Face `Trainer` API
- Swap in your custom model

## File Structure

```
backend/
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py (UPDATED - now uses DistilBERT)
â”‚   â””â”€â”€ sentiment_classifier.py (NEW - DistilBERT wrapper)
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt (UPDATED - added transformers, torch)
â””â”€â”€ ...
```

## Next Steps

1. âœ… Test the sentiment analysis with real YouTube comments
2. âœ… Monitor prediction accuracy on your dashboard
3. âœ… Fine-tune thresholds based on business feedback
4. ðŸ”„ (Optional) Fine-tune the model on domain-specific data

## Questions?

Refer to:

- Hugging Face Transformers: https://huggingface.co/transformers/
- DistilBERT Paper: https://arxiv.org/abs/1910.01108
- Sentiment Models: https://huggingface.co/models?pipeline_tag=sentiment-analysis
